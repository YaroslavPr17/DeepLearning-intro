# -*- coding: utf-8 -*-
"""homework_8_83_try_scheduler_stops_8_83_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/159i3fxCXjM23cPQS88jVNgLR-SB8HHKx

# Основы глубинного обучения, майнор ИАД

## Домашнее задание 1. Введение в PyTorch. Полносвязные нейронные сети.

### Общая информация

Дата выдачи: 20.09.2022

Мягкий дедлайн: 23:59MSK 04.10.2022

Жесткий дедлайн: 23:59MSK 10.10.2022

### Оценивание и штрафы
Максимально допустимая оценка за работу — 10 баллов. За каждый день просрочки снимается 1 балл. Сдавать задание после жёсткого дедлайна сдачи нельзя.

Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).

Неэффективная реализация кода может негативно отразиться на оценке.
Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.

### О задании

В этом задании вам предстоит предсказывать год выпуска песни (**задача регрессии**) по некоторым звуковым признакам: [данные](https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd). В ячейках ниже находится код для загрузки данных. Обратите внимание, что обучающая и тестовая выборки располагаются в одном файле, поэтому НЕ меняйте ячейку, в которой производится деление данных.
"""

import torch
from torch import nn
import pandas as pd
import numpy as np
import random

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer 
import matplotlib.pyplot as plt
import seaborn as sns

from typing import Union, Any, Dict
from tqdm import tqdm

SEED = 123

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"device = '{device}'")


# !wget -O data.txt.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip

df = pd.read_csv('data.txt.zip', header=None)
df

# DO = nn.Dropout(p=0.3)
# df = pd.DataFrame(DO(torch.tensor(df.values, device=device)))


X = df.iloc[:, 1:].values
y = df.iloc[:, 0].values

train_size = 463715
X_train = X[:train_size, :]
y_train = y[:train_size]
X_test = X[train_size:, :]
y_test = y[train_size:]

"""###### Source: [askpython.com](https://www.askpython.com/python/examples/detection-removal-outliers-in-python)"""

# sns.boxplot(x=X_train[:, 7])
# plt.show()

# for col_index in range(X_train.shape[1]): 
#     q75,q25 = np.percentile(X_train[:, col_index], [10,90])
#     intr_qr = q75-q25

#     max = q75+(1.5*intr_qr)
#     min = q25-(1.5*intr_qr)

#     X_train[:, col_index][X_train[:, col_index] > min] = np.nan
#     X_train[:, col_index][X_train[:, col_index] < max] = np.nan

# # Замена nan на среднее
# print(np.isnan(X_train).sum())
# imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
# X_train = imp_mean.fit_transform(X_train)

# sns.boxplot(x=X_train[:, 7])
# plt.show()

"""---

## Задание 0. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)

Мы будем использовать RMSE как метрику качества. Для самого первого бейзлайна обучите `Ridge` регрессию из `sklearn`. Кроме того, посчитайте качество при наилучшем константном прогнозе.
"""

print(X_train.shape)
X_train

print(y_train.shape)
y_train

# alphas = [value for value in np.linspace(10e-64, 10e6, 32)]
# rates = []
# for alpha in tqdm(alphas):
#   model = Ridge(alpha=alpha)
#   model.fit(X_train, y_train)
#   rates.append(mean_squared_error(y_test, model.predict(X_test), squared=False))

# best_alpha = alphas[np.argmin(rates)]

# model = Ridge(alpha=best_alpha)
# model.fit(X_train, y_train)
# ridge_rmse = mean_squared_error(y_test, model.predict(X_test), squared=False)
  
# plt.plot(alphas, rates)
# plt.plot(best_alpha, ridge_rmse, 'bo')
# plt.title("RMSE for every alpha", fontWeight="bold")
# plt.xlabel("alpha")
# plt.ylabel("RMSE")
# plt.show()

# print("For Ridge(alpha=%2.e) RMSE = %.2f" % (best_alpha, ridge_rmse))

# y_const_predict = np.full_like(y_test, np.mean(y_train))
# print("For *const* RMSE = %.2f" % (mean_squared_error(y_test, y_const_predict, squared=False)))

"""---

# **EDA**
"""

# df

# df_ = df.copy()
# for col in df.columns:
#     df_[col] = (df_[col])

# plt.figure(figsize=(20, 25))

# sns.heatmap(pd.DataFrame(data=X_train).corr())
# plt.show()

# X_train = np.delete(X_train, range(13, 23), axis=1)
# X_test = np.delete(X_test, range(13, 23), axis=1)
# print(X_train.shape)

"""---

# Задание 1. (максимум 10 баллов)

Реализуйте обучение и тестирование нейронной сети для предоставленного вам набора данных. Соотношение между полученным значением метрики на тестовой выборке и баллами за задание следующее:

- $\text{RMSE} \le 9.00 $ &mdash; 4 балла
- $\text{RMSE} \le 8.90 $ &mdash; 6 баллов
- $\text{RMSE} \le 8.80 $ &mdash; 8 баллов
- $\text{RMSE} \le 8.75 $ &mdash; 10 баллов

Есть несколько правил, которых вам нужно придерживаться:

- Весь пайплайн обучения должен быть написан на PyTorch. При этом вы можете пользоваться другими библиотеками (`numpy`, `sklearn` и пр.), но только для обработки данных. То есть как угодно трансформировать данные и считать метрики с помощью этих библиотек можно, а импортировать модели из `sklearn` и выбивать с их помощью требуемое качество &mdash; нельзя. Также нельзя пользоваться библиотеками, для которых сам PyTorch является зависимостью.

- Мы никак не ограничиваем ваш выбор архитектуры модели, но скорее всего вам будет достаточно полносвязной нейронной сети.

- Для обучения запрещается использовать какие-либо иные данные, кроме обучающей выборки.

- Ансамблирование моделей запрещено.

### Полезные советы:

- Очень вряд ли, что у вас с первого раза получится выбить качество на 10 баллов, поэтому пробуйте разные архитектуры, оптимизаторы и значения гиперпараметров. В идеале при запуске каждого нового эксперимента вы должны менять что-то одно, чтобы точно знать, как этот фактор влияет на качество.

- Не забудьте, что для улучшения качества модели вам поможет **нормировка таргета**.

- Тот факт, что мы занимаемся глубинным обучением, не означает, что стоит забывать про приемы, использующиеся в классическом машинном обучении. Так что обязательно проводите исследовательский анализ данных, отрисовывайте нужные графики и не забывайте про масштабирование и подбор гиперпараметров.

- Вы наверняка столкнетесь с тем, что ваша нейронная сеть будет сильно переобучаться. Для нейросетей существуют специальные методы регуляризации, например, dropout ([статья](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)) и weight decay ([блогпост](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd)). Они, разумеется, реализованы в PyTorch. Попробуйте поэкспериментировать с ними.

- Если вы чего-то не знаете, не гнушайтесь гуглить. В интернете очень много полезной информации, туториалов и советов по глубинному обучению в целом и по PyTorch в частности. Но не забывайте, что за скатанный код без ссылки на источник придется ответить по всей строгости!

- Если вы сразу реализуете обучение на GPU, то у вас будет больше времени на эксперименты, так как любые вычисления будут работать быстрее. Google Colab предоставляет несколько GPU-часов (обычно около 8-10) в сутки бесплатно.

- Чтобы отладить код, можете обучаться на небольшой части данных или даже на одном батче. Если лосс на обучающей выборке не падает, то что-то точно идет не так!

- Пользуйтесь утилитами, которые вам предоставляет PyTorch (например, Dataset и Dataloader). Их специально разработали для упрощения разработки пайплайна обучения.

- Скорее всего вы захотите отслеживать прогресс обучения. Для создания прогресс-баров есть удобная библиотека `tqdm`.

- Быть может, вы захотите, чтобы графики рисовались прямо во время обучения. Можете воспользоваться функцией [clear_output](http://ipython.org/ipython-doc/dev/api/generated/IPython.display.html#IPython.display.clear_output), чтобы удалять старый график и рисовать новый на его месте.

**ОБЯЗАТЕЛЬНО** рисуйте графики зависимости лосса/метрики на обучающей и тестовой выборках в зависимости от времени обучения. Если обучение занимает относительно небольшое число эпох, то лучше рисовать зависимость от номера шага обучения, если же эпох больше, то рисуйте зависимость по эпохам. Если проверяющий не увидит такого графика для вашей лучшей модели, то он в праве снизить баллы за задание.

**ВАЖНО!** Ваше решение должно быть воспроизводимым. Если это не так, то проверяющий имеет право снизить баллы за задание. Чтобы зафиксировать random seed, воспользуйтесь функцией из ячейки ниже.
"""

def set_random_seed(seed):
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

set_random_seed(SEED)

"""Вы можете придерживаться любой адекватной струкуры кода, но мы советуем воспользоваться следующими сигнатурами функций. Лучше всего, если вы проверите ваши предсказания ассертом: так вы убережете себя от разных косяков, например, что вектор предсказаний состоит из всего одного числа. В любом случае, внимательно следите за тем, для каких тензоров вы считаете метрику RMSE. При случайном или намеренном введении в заблуждение проверяющие очень сильно разозлятся."""

set_random_seed(SEED)

MEAN = np.mean(y_train)
STD = np.std(y_train)

scaler = StandardScaler()

scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)
scaler = StandardScaler().fit(y_train)
y_train = scaler.transform(y_train)
y_test = scaler.transform(y_test)

# set_random_seed(SEED)

# MIN = np.min(y_train)
# MAX = np.max(y_train)

# scaler = MinMaxScaler()

# scaler.fit(X_train)
# X_train = scaler.transform(X_train)
# X_test = scaler.transform(X_test)

# y_train = y_train.reshape(-1, 1)
# y_test = y_test.reshape(-1, 1)
# scaler = MinMaxScaler().fit(y_train)
# y_train = scaler.transform(y_train)
# y_test = scaler.transform(y_test)

y_train

X_train = torch.tensor(X_train, dtype=torch.float, requires_grad=True, device=device)
X_test = torch.tensor(X_test, dtype=torch.float, requires_grad=True, device=device)
y_train = torch.tensor(y_train, dtype=torch.float, device=device)
y_test = torch.tensor(y_test, dtype=torch.float, device=device)

f"{y_train.size()} {X_train.size()} \n {y_test.size()} {X_test.size()}"

class MusicDataset(torch.utils.data.Dataset):
    """Dataset of music parameters"""
    def __init__(self, X: torch.Tensor, y: torch.Tensor) -> None:
      super().__init__()
      self.X :torch.Tensor = X
      self.y :torch.Tensor = y

    def __len__(self) -> int:
        return self.X.size()[0]

    def __getitem__(self, idx: int):
      return {'sample': self.X[idx, :], 'target': self.y[idx]}

val_res = []

set_random_seed(SEED)

train_set = MusicDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=64)

test_set = MusicDataset(X_test, y_test)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=64)

test_set_for_val_test = MusicDataset(X_test, y_test)
test_loader_for_val_test = torch.utils.data.DataLoader(test_set_for_val_test, batch_size=len(test_set_for_val_test))

set_random_seed(SEED)

model = nn.Sequential(
    nn.Linear(90, 128),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(128, 256),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(256, 512),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(32, 16),
    nn.ReLU(),
    nn.Linear(16, 8),
    nn.ReLU(),
    nn.Linear(8, 4),
    nn.ReLU(),
    nn.Linear(4, 1)
).to(device=device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.0023)

# lambda_func = lambda epoch: 0.9 ** epoch
# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)

scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)

# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)


criterion = nn.MSELoss()

def train(n_epochs, model, optimizer, criterion, scheduler, train_loader, test_loader=None, scaling: bool =True):
    '''
    params:
        model - torch.nn.Module to be fitted
        optimizer - model optimizer
        criterion - loss function from torch.nn
        train_loader - torch.utils.data.Dataloader with train set
        test_loader - torch.utils.data.Dataloader with test set
                      (if you wish to validate during training)
    '''

    if n_epochs <= 0:
        raise ValueError('Number of epochs should be positive, not %d.' % (n_epochs))

    rmse_s = []
    batches = []
    n_batch = 0
    current_min = 10e8

    try:         

        for epoch in range(n_epochs):
            set_random_seed(SEED)
            print("\nEpoch = %d" % (epoch))
            for n, batch in enumerate(tqdm(train_loader)):
                X_train_epoch, y_train_epoch = batch['sample'], batch['target']
                y_pred = model(X_train_epoch)

                loss = torch.sqrt(criterion(y_pred, y_train_epoch))
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

                if n % 20 == 0:
                    predicts = test(model, criterion, test_loader)
                    val_res.append(mean_squared_error(scaler.inverse_transform(y_test.cpu().detach()),
                                                      scaler.inverse_transform(predicts.cpu().detach()), squared=False))


            if np.min(val_res) < current_min:
                current_min = np.min(val_res)
            if current_min < 8.75:
                raise KeyboardInterrupt
            scheduler.step()


            if test_loader is not None:
                predicts = test(model, criterion, test_loader)
                
                if scaling == True:
                    rmse_ = mean_squared_error(scaler.inverse_transform(y_test.cpu().detach()), scaler.inverse_transform(predicts.cpu().detach()), squared=False)

                else:
                    rmse_ = mean_squared_error(y_test.cpu().detach(), predicts.cpu().detach(), squared=False)
                print(rmse_)
                print("Min value:", current_min)
                rmse_s.append(rmse_)

                batches.append(n_batch)
                n_batch += 1
    
    except KeyboardInterrupt:  
        pass
        
    finally:              
        if test_loader is not None and len(batches) > 1:
            plt.plot(batches, rmse_s)
            plt.title("Test-loss for every epoch while training. " + str(len(batches)) + " epochs.")
            plt.xlabel("Epoch number")
            plt.ylabel("RMSE")


def test(model, criterion, test_loader) -> torch.Tensor:
    '''
    params:
        model - torch.nn.Module to be evaluated on test set
        criterion - loss function from torch.nn
        test_loader - torch.utils.data.Dataloader with test set
    ----------
    returns:
        predicts - torch.tensor with shape (len(test_loader.dataset), ),
                   which contains predictions for test objects
    '''
    predicts = torch.tensor([], device=device)
    for batch in test_loader:
        X_tst, y_tst = batch['sample'], batch['target']
        predicts = torch.cat((predicts, model(X_tst)))        

    return predicts

# # 32, 45, Adam, 0.0023
# train(1, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # 32, 45, Adagrad, 0.01
# train(1, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # 8, 45, Adam, 0.0023
# train(1, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 32, 1 скрытый слой, просто подождал
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# #
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 32, слоёв чуть больше
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 32, max слоёв
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# """## Best result!"""
#
# # batch_size = 64, max слоёв
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, weight_decay
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, weight_decay, MinMaxScaler
# train(5, model, optimizer, criterion, train_loader, test_loader, 'mm')
#
# # batch_size = 64, max слоёв, Без взаимосвязанных признаков
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, замена выбросов
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0038
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0045
# train(5, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, weight_decay=0.0001
# train(7, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), Замена выбросов на среднее (25,75)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), Замена выбросов на среднее (10,90)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), Удаление зависимых признаков
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (180 - 256 - 512 - 1024 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), 181 признак (+квадраты)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 128 - 256 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 90 - 90 - 90 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, max слоёв, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 1024 - 512 - 256 - 128 - 78 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # Same as previous, but batch_size=32
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # Where 8.83, but batch_size=128
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # Where 8.83, lr=0.0038
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 1024 - 1024 - 1024 - 1024 - 1024 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, 'std')
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1)
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-lambda(0.95) - sch every epoch!!!
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # Результат как без scheduler'a
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-Exponential(0.9) - sch every epoch!!!
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-Exponential(0.7) - sch every epoch!!!
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-StepLR(0.8) - sch every 1000 batches
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-StepLR(0.8) - sch every 1000 batches
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, more neurons then input (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.8) - sch every epoch
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.005, (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.9) - sch every epoch
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.004, (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.9) - sch every epoch
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.002, (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.9) - sch every epoch
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0025, (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.9) - sch every epoch
# train(10, model, optimizer, criterion, train_loader, test_loader, True)
#
# # batch_size = 64, lr=0.0023, (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.9) - sch every epoch
# train(10, model, optimizer, criterion, train_loader, test_loader, True)

# batch_size = 64, lr=0.0023, (90 - 128 - 256 - 512 - 256 - 128 - 64 - 32 - 16 - 8 - 4 - 1), scheduler-LambdaLR(0.9) - sch every epoch
train(15, model, optimizer, criterion, scheduler, train_loader, test_loader, True)

print(np.min(val_res))

file = open("magic.txt", "wt")
print(np.min(val_res), file=file)
print(val_res, file=file)


# res = test(model, criterion, test_loader_for_val_test)

# set_random_seed(SEED)
#
# model = nn.Sequential(
#     nn.Linear(90, 128),
#     nn.ReLU(),
#     nn.Linear(128, 256),
#     nn.ReLU(),
#     nn.Linear(256, 512),
#     nn.ReLU(),
#     nn.Dropout(0.3),
#     nn.Linear(512, 256),
#     nn.ReLU(),
#     nn.Linear(256, 128),
#     nn.ReLU(),
#     nn.Dropout(0.3),
#     nn.Linear(128, 64),
#     nn.ReLU(),
#     nn.Dropout(0.2),
#     nn.Linear(64, 32),
#     nn.ReLU(),
#     nn.Dropout(0.2),
#     nn.Linear(32, 16),
#     nn.ReLU(),
#     nn.Linear(16, 8),
#     nn.ReLU(),
#     nn.Linear(8, 4),
#     nn.ReLU(),
#     nn.Linear(4, 1)
# ).to(device=device)
#
# optimizer = torch.optim.Adam(model.parameters(), lr=0.0023, weight_decay=0.00002)
#
# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)
#
# train(20, model, optimizer, criterion, scheduler, train_loader, test_loader, True)











# res
#
# res * STD + MEAN
#
# mean_squared_error((y_test * STD + MEAN).cpu().detach(), (res * STD + MEAN).cpu().detach(), squared=False)
#
# y_test * STD + MEAN

"""# Задание 2. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)

Напишите небольшой отчет о том, как вы добились полученного качества: какие средства использовали и какие эксперименты проводили. Подробно расскажите об архитектурах и значениях гиперпараметров, а также какие метрики на тесте они показывали. Чтобы отчет был зачтен, необходимо привести хотя бы 3 эксперимента.
"""

# YOUR CODE HERE (－.－)...zzzZZZzzzZZZ

